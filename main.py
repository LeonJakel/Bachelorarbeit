# -*- coding: utf-8 -*-
"""bachelor_week1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he_Axo2JbQu2lwzUYV_ABINXC47QODT4
"""

"""
!nvidia-smi

!gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT

!unzip -qq ECG5000.zip

!pip install -qq pandas

!pip install -qq scipy
torch
"""

from scipy.io import arff

# Commented out IPython magic to ensure Python compatibility.
import torch
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch import nn, optim
import time

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

CLASS_NORMAL = 1


def plot_time_series_class(data, class_name, ax, n_steps=10):
    time_series_df = pd.DataFrame(data)

    smooth_path = time_series_df.rolling(n_steps).mean()
    path_deviation = 2 * time_series_df.rolling(n_steps).std()

    under_line = (smooth_path - path_deviation)[0]
    over_line = (smooth_path + path_deviation)[0]

    ax.plot(smooth_path, linewidth=2)
    ax.fill_between(
        path_deviation.index,
        under_line,
        over_line,
        alpha=.125
    )
    ax.set_title(class_name)


def create_dataset(sequences):
    dataset = [torch.tensor(s).unsqueeze(1) for s in sequences]

    n_seq, seq_len, n_features = torch.stack(dataset).shape

    return dataset, seq_len, n_features


"""##Building an LSTM Autoencoder"""


class Encoder(nn.Module):

    def __init__(self, seq_len, n_features, embedding_dim=64):
        super(Encoder, self).__init__()
        self.seq_len, self.n_features = seq_len, n_features
        self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim
        self.rnn1 = nn.LSTM(
            input_size=n_features,
            hidden_size=self.hidden_dim,
            num_layers=1,
            batch_first=True
        )

        self.rnn2 = nn.LSTM(
            input_size=self.hidden_dim,
            hidden_size=embedding_dim,
            num_layers=1,
            batch_first=True
        )

    def forward(self, x):
        x = x.reshape((1, self.seq_len, self.n_features))
        x, (_, _) = self.rnn1(x)
        x, (hidden_n, _) = self.rnn2(x)
        return hidden_n.reshape((self.n_features, self.embedding_dim))


class Decoder(nn.Module):

    def __init__(self, seq_len, input_dim=64, n_features=1):
        super(Decoder, self).__init__()
        self.seq_len, self.input_dim = seq_len, input_dim
        self.hidden_dim, self.n_features = 2 * input_dim, n_features
        self.rnn1 = nn.LSTM(
            input_size=input_dim,
            hidden_size=input_dim,
            num_layers=1,
            batch_first=True
        )

        self.rnn2 = nn.LSTM(
            input_size=input_dim,
            hidden_size=self.hidden_dim,
            num_layers=1,
            batch_first=True
        )

        self.output_layer = nn.Linear(self.hidden_dim, n_features)

    def forward(self, x):
        x = x.repeat(self.seq_len, self.n_features)
        x = x.reshape((self.n_features, self.seq_len, self.input_dim))
        x, (hidden_n, cell_n) = self.rnn1(x)
        x, (hidden_n, cell_n) = self.rnn2(x)
        x = x.reshape((self.seq_len, self.hidden_dim))
        return self.output_layer(x)


class RecurrentAutoencoder(nn.Module):

    def __init__(self, seq_len, n_features, embedding_dim=64):
        super(RecurrentAutoencoder, self).__init__()
        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)
        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


"""##Training"""


def training(model, train_dataset, val_dataset, epochs):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.L1Loss(reduction='sum').to(device)

    history = dict(train=[], val=[])

    start_time = time.time()
    for epoch in range(1, epochs + 1):
        model = model.train()

        train_losses = []

        for seq_true in train_dataset:
            optimizer.zero_grad()
            seq_true = seq_true.to(device)
            seq_pred = model(seq_true)

            loss = criterion(seq_pred, seq_true)

            loss.backward()
            optimizer.step()

            train_losses.append(loss.item())

        val_losses = []
        model = model.eval()
        with torch.no_grad():
            for seq_true in val_dataset:
                seq_true = seq_true.to(device)
                seq_pred = model(seq_true)

                loss = criterion(seq_pred, seq_true)

                val_losses.append(loss.item())

        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)

        history['train'].append(train_loss)
        history['val'].append(val_loss)

        end_time = time.time()
        print(f'Epoch {epoch}: train Loss {train_loss} | val Loss {val_loss} | elapsed time {round(end_time-start_time, 2)}s' )

    return model.eval(), history


"""##Chosing a threshold"""


def predict(model, dataset):
    prediction, losses = [], []
    criterion = nn.L1Loss(reduction='sum').to(device)

    with torch.no_grad():
        model = model.eval()
        for seq_true in dataset:
            seq_true = seq_true.to(device)
            seq_pred = model(seq_true)

            loss = criterion(seq_pred, seq_true)

            prediction.append(seq_pred.cpu().numpy())
            losses.append(loss.item())

    return prediction, losses


def load_files(train_file, test_file):

    f = arff.loadarff('data/' + train_file)
    train = pd.DataFrame(f[0])

    if test_file != '0':
        f = arff.loadarff('data/' + test_file)
        test = pd.DataFrame(f[0])
        df = pd.concat([train, test]) #concat train- and testfile
    else:
        df = train

    df = df.sample(frac=1.0)  #randomize df

    df['target'] = df['target'].astype(str)  # remove b' ' from target collumn

    return df


def data_preprocessing(df, train_split_ratio, val_split_ratio):

    normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)

    # print(normal_df.shape)

    anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)

    # print(anomaly_df.shape)

    train_df, val_df = train_test_split(normal_df, train_size=train_split_ratio, random_state=RANDOM_SEED)

    val_df, test_df = train_test_split(val_df, train_size=val_split_ratio, random_state=RANDOM_SEED)

    train_sequences = train_df.astype(np.float32).to_numpy().tolist()
    val_sequences = val_df.astype(np.float32).to_numpy().tolist()
    test_sequences = test_df.astype(np.float32).to_numpy().tolist()
    anomaly_sequences = anomaly_df.astype(np.float32).to_numpy().tolist()

    train_dataset, seq_len, n_features = create_dataset(train_sequences)
    val_dataset, seq_len, n_features = create_dataset(val_sequences)
    test_normal_dataset, seq_len, n_features = create_dataset(test_sequences)
    test_anomaly_dataset, seq_len, n_features = create_dataset(anomaly_sequences)

    return train_dataset, val_dataset, test_normal_dataset,test_anomaly_dataset, seq_len, n_features
#Main

def main():
    start_time = time.time()

    #config
    train_file = 'ECG5000_TRAIN.arff'
    test_file = 'ECG5000_TEST.arff'
    train_split_ratio = 0.85
    val_split_ratio = 0.5
    embedding_size= 128
    epochs = 2
    model_path = 'model.pth'
    model = "RAE"
    threshold = 26

    df = load_files(train_file, test_file) #load both files into panda dataframe

    train_dataset, val_dataset, test_normal_dataset, test_anomaly_dataset, seq_len, n_features = data_preprocessing \
        (df, train_split_ratio, val_split_ratio)
    if model == "RAE":
        model = RecurrentAutoencoder(seq_len, n_features, embedding_dim=embedding_size).to(device)
    elif model == "FF":
        print("not implemented yet")
        exit(-1)
    else:
        print("no model selected")
        exit(-1)

    model, history = training(model, train_dataset, val_dataset, epochs=epochs)

    #Saving the Model
    torch.save(model, model_path)
    _, losses = predict(model, train_dataset)

    #Evaluation
    predictions, pred_losses = predict(model, test_normal_dataset)

    correct = sum(l <= threshold for l in pred_losses)
    print(f'Correct normal predictions: {correct}/{len(test_normal_dataset)}')

    anomaly_dataset = test_anomaly_dataset[:len(test_normal_dataset)]
    predictions, pred_losses = predict(model, anomaly_dataset)
    correct = sum(l > threshold for l in pred_losses)
    print(f'Correct anomaly predictions: {correct}/{len(anomaly_dataset)}')

    end_time = time.time()
    print(f'Total time to finish: {round(end_time-start_time, 2)}')
if __name__ == '__main__':
    main()
